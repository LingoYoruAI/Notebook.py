# -*- coding: utf-8 -*-
"""YP.ipynb Bert

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nRzaXZltb6EQu02LPhIQpy7d0J6d__ar

# **Import** **Necessary** **Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""# **Load** **Dataset**"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Combined_Proverbs.csv')

"""# **Data** **Review**"""

df.head(4)

df.shape

# Checking for duplicate rows in the DataFrame
df.duplicated().sum()

# Drop duplicates rows
df = df.drop_duplicates()

df.duplicated().sum()

"""# **Text** **Pre**-processing"""

import re
from nltk.stem import PorterStemmer

# Custom Yoruba stopword list
yoruba_stopwords = ['ó', 'ní', 'ṣe', 'rẹ̀', 'tí', 'àwọn', 'sí', 'ni', 'náà',
                    'láti', 'kan', 'ti', 'ń', 'lọ', 'o', 'bí', 'padà', 'sì',
                    'wá', 'lè', 'wà', 'kí', 'púpọ̀', 'mi', 'wọ́n', 'pẹ̀lú',
                    'a', 'ṣùgbọ́n', 'fún', 'jẹ́', 'fẹ́', 'kò', 'jù', 'pé',
                    'é', 'gbogbo', 'inú', 'bẹ̀rẹ̀', 'jẹ', 'nítorí', 'nǹkan',
                    'sínú', 'yìí', 'ṣé', 'àti', 'í', 'máa', 'nígbà', 'mo',
                    'an', 'mọ̀', 'bá', 'kì', 'ńlá', 'ọ̀pọ̀lọpọ̀', 'ẹmọ́',
                    'wọn', 'òun']

stop_words = list(yoruba_stopwords)

# Initialize the stemmer
stemmer = PorterStemmer()

# Define function to remove URLs, HTML tags, stopwords, and apply stemming

def clean_text(text):
    # Check if input is a list and join into a string if necessary
    if isinstance(text, list):
        text = ' '.join(text)
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)

    # Tokenize the text
    words = text.split()

    # Remove stopwords and apply stemming
    filtered_and_stemmed_words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]

    # Join the filtered and stemmed words back into a sentence
    return ' '.join(filtered_and_stemmed_words)

# Apply the clean_text function to each row in the 'Proverbs' column
df['Proverbs'] = df['Proverbs'].apply(clean_text)

df.head()

from sklearn.model_selection import train_test_split

# Extract features and labels
texts = df['Proverbs']
labels = df['Labels']

# Train-test split
train_texts, test_texts, train_labels, test_labels = train_test_split(
    texts, labels, test_size=0.2, random_state=42
)

from transformers import BertTokenizer

# Load tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")

# Define tokenization function
def tokenize_texts(texts, tokenizer, max_len=50):
    return tokenizer(
        list(texts),
        add_special_tokens=True,
        max_length=max_len,
        padding="max_length",
        truncation=True,
        return_tensors="pt",
    )

# Tokenize training and testing data
train_encodings = tokenize_texts(train_texts, tokenizer)
test_encodings = tokenize_texts(test_texts, tokenizer)

import torch

class YorubaProverbDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: tensor[idx] for key, tensor in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)
        return item

# Create datasets
train_dataset = YorubaProverbDataset(train_encodings, train_labels)
test_dataset = YorubaProverbDataset(test_encodings, test_labels)

from transformers import BertModel
import torch.nn as nn

class ProverbClassifier(nn.Module):
    def __init__(self, n_classes):
        super(ProverbClassifier, self).__init__()
        self.bert = BertModel.from_pretrained("bert-base-multilingual-cased")
        self.drop = nn.Dropout(p=0.3)
        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        output = self.drop(pooled_output)
        return self.out(output)

model = ProverbClassifier(n_classes=2)

from torch.utils.data import DataLoader
from transformers import AdamW, get_scheduler
from torch.nn import CrossEntropyLoss
import torch

# DataLoaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

# Optimizer and scheduler
optimizer = AdamW(model.parameters(), lr=5e-5)
num_training_steps = len(train_loader) * 5  # 5 epochs
lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)

# Set device
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

# Define loss function
loss_fn = CrossEntropyLoss()

# Training loop
num_epochs = 5  #  5 epochs
for epoch in range(num_epochs):
    print(f"Epoch {epoch + 1}/{num_epochs}")
    model.train()
    epoch_loss = 0  #  epoch loss

    for batch in train_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = loss_fn(outputs, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        lr_scheduler.step()

        # Accumulate loss
        epoch_loss += loss.item()

    avg_loss = epoch_loss / len(train_loader)  # Average loss for this epoch
    print(f"Average Loss: {avg_loss:.4f}")

print("Training complete!")

from sklearn.metrics import accuracy_score, classification_report

# Set model to evaluation mode
model.eval()

# Initialize variables to store predictions and true labels
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs, dim=1)  # Get predicted class

        # Store predictions and labels
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calculate accuracy
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy:.4f}")

# Generate classification report
print("Classification Report:")
print(classification_report(all_labels, all_preds, target_names=["Non-Proverb", "Proverb"]))

from sklearn.metrics import confusion_matrix

# Set model to evaluation mode
model.eval()

# Initialize variables to store predictions and true labels
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs, dim=1)  # Get predicted class

        # Store predictions and labels
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Generate confusion matrix
tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()

# Print the confusion matrix values
print(f"True Positive (TP): {tp}")
print(f"True Negative (TN): {tn}")
print(f"False Positive (FP): {fp}")
print(f"False Negative (FN): {fn}")

from torch.utils.data import DataLoader
from transformers import AdamW, get_scheduler
from torch.nn import CrossEntropyLoss
import torch

# Optimizer and scheduler for fine-tuning
optimizer = AdamW(model.parameters(), lr=1e-5)  # Fine-tuning with smaller learning rate
num_finetune_steps = len(train_loader) * 2  # Fine-tuning for 2 more epochs
lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_finetune_steps)

# Set the device
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

# Fine-tuning loop
num_finetune_epochs = 2  # Fine-tune for 2 epochs
for epoch in range(num_finetune_epochs):
    print(f"Fine-tuning Epoch {epoch + 1}/{num_finetune_epochs}")
    model.train()
    epoch_loss = 0  # Track epoch loss

    for batch in train_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = CrossEntropyLoss()(outputs, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        lr_scheduler.step()

        # Accumulate loss
        epoch_loss += loss.item()

    avg_loss = epoch_loss / len(train_loader)  # Average loss for this epoch
    print(f"Average Loss: {avg_loss:.4f}")

print("Fine-tuning complete!")

# Set model to evaluation mode after fine-tuning
model.eval()

# Initialize variables to store predictions and true labels
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs, dim=1)  # Get predicted class

        # Store predictions and labels
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Generate confusion matrix
tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()

# Print the confusion matrix values
print(f"True Positive (TP): {tp}")
print(f"True Negative (TN): {tn}")
print(f"False Positive (FP): {fp}")
print(f"False Negative (FN): {fn}")

from sklearn.metrics import accuracy_score

# Set model to evaluation mode
model.eval()

# Initialize variables to store predictions and true labels
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs, dim=1)  # Get predicted class

        # Store predictions and labels
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calculate accuracy
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy * 100:.2f}%")

from sklearn.metrics import accuracy_score, classification_report

# Set model to evaluation mode
model.eval()

# Initialize variables to store predictions and true labels
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs, dim=1)  # Get predicted class

        # Store predictions and labels
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calculate accuracy
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Print classification report
print(classification_report(all_labels, all_preds))

from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import numpy as np

# Set model to evaluation mode
model.eval()

# Initialize variables to store predictions, true labels, and probabilities
all_preds = []
all_labels = []
all_probs = []  # Initialize all_probs

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        probs = torch.softmax(outputs, dim=1) # Get predicted probabilities
        preds = torch.argmax(outputs, dim=1)  # Get predicted class

        # Store predictions, labels, and probabilities
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        all_probs.extend(probs.cpu().numpy()) # Store probabilities

# Calculate accuracy
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Print classification report
print(classification_report(all_labels, all_preds))

positive_probs = [prob[1] for prob in all_probs]  # Extract probabilities for class 1

# Calculate AUC using the positive class probabilities
auc = roc_auc_score(all_labels, positive_probs, average='macro')
print(f"AUC: {auc:.2f}")

from transformers import BertTokenizer
import torch

# Define the model architecture to match the saved model
class ProverbClassifier(torch.nn.Module):
    def __init__(self):
        super(ProverbClassifier, self).__init__()
        from transformers import BertModel
        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')
        self.out = torch.nn.Linear(self.bert.config.hidden_size, 2)
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        logits = self.out(outputs.pooler_output)
        return logits

# Load the saved model and tokenizer
model = ProverbClassifier()

# Load model weights
model.load_state_dict(torch.load("proverb_classifier_weights.pth"))
model.eval()  # Set the model to evaluation mode

# Load the tokenizer (use the same tokenizer used during training)
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

# New input (Yoruba proverb or text you want to classify)
new_text = """
Ilé ìfowópamọ́ tó wà ní iwájú ilé wà ni mò ń lọ.
Mo fẹ́ràn aṣọ tí arábìnrin yìí wọ̀.
Ológbò akọ àti abo ni mò ń sìn.
Ẹni tó bá sọ pé ọ̀rọ̀ àwọn ìjọba yìí ò sún òun, ọdájú ni ẹni náà.
Ebi ń pamí mo fẹ́ jẹun.
Ìgbáyàwó tí mo lọ lánàá yẹn dùn gan.
Ọkọ mi ń lọ sí ilẹ̀ òkèrè fún iṣẹ́.
Ìròyìn àwọn Ọlọ́pàá yẹn tí kàn sí wa.
Ọmọ mélòó lo fẹ́ bí láyé rẹ.
Ìgbà wo ni bàbá rẹ máa dé láti ibiṣẹ́.  # Example Yoruba proverb
"""

# Tokenize the input text
inputs = tokenizer(new_text, return_tensors="pt", padding=True, truncation=True, max_length=512)

# Move the inputs to the same device as the model (GPU or CPU)
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
inputs = {key: value.to(device) for key, value in inputs.items()}
model.to(device)

# Make predictions with the model
with torch.no_grad():
    # Forward pass
    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])

    # If the model directly returns logits as tensor
    preds = torch.argmax(outputs, dim=1)

# Print the predicted class
if preds.item() == 1:
    print("Prediction: Proverb")
else:
    print("Prediction: Non-Proverb")

from transformers import BertTokenizer
import torch

# Define the model
class ProverbClassifier(torch.nn.Module):
    def __init__(self):
        super(ProverbClassifier, self).__init__()
        from transformers import BertModel
        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')
        self.out = torch.nn.Linear(self.bert.config.hidden_size, 2)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        logits = self.out(outputs.pooler_output)
        return logits

# Load the saved model and tokenizer
model = ProverbClassifier()
model.load_state_dict(torch.load("proverb_classifier_weights.pth"))  # Load the model weights
model.eval()  # Set the model to evaluation mode

# Load the tokenizer (use the same tokenizer used during training)
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

# List of new input texts (Yoruba proverbs or texts you want to classify)
texts = [
    "Ilé ìfowópamọ́ tó wà ní iwájú ilé wà ni mò ń lọ.",
    "Mo fẹ́ràn aṣọ tí arábìnrin yìí wọ̀.",
    "Ológbò akọ àti abo ni mò ń sìn.",
    "Ẹni tó bá sọ pé ọ̀rọ̀ àwọn ìjọba yìí ò sún òun, ọdájú ni ẹni náà.",
    "Ebi ń pamí mo fẹ́ jẹun.",
    "Ìgbáyàwó tí mo lọ lánàá yẹn dùn gan.",
    "Ọkọ mi ń lọ sí ilẹ̀ òkèrè fún iṣẹ́.",
    "Ìròyìn àwọn Ọlọ́pàá yẹn tí kàn sí wa.",
    "Ọmọ mélòó lo fẹ́ bí láyé rẹ.",
    "Ìgbà wo ni bàbá rẹ máa dé láti ibiṣẹ́."  # Example Yoruba proverbs
]

# Tokenize the input texts and classify them
for new_text in texts:
    # Tokenize the input text
    inputs = tokenizer(new_text, return_tensors="pt", padding=True, truncation=True, max_length=512)

    # Move the inputs to the same device as the model (GPU or CPU)
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    inputs = {key: value.to(device) for key, value in inputs.items()}
    model.to(device)

    # Make predictions with the model
    with torch.no_grad():
        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])

        # Get predicted class label (0: Non-Proverb, 1: Proverb)
        preds = torch.argmax(outputs, dim=1)

    # Print the result
    if preds.item() == 1:
        print(f"Text: {new_text} \nPrediction: Proverb\n")
    else:
        print(f"Text: {new_text} \nPrediction: Non-Proverb\n")

# Save the entire model
torch.save(model, "proverb_classifier_full.pth")
print("Model saved!")

torch.save(model.state_dict(), "proverb_classifier_weights.pth")

# Save the  model
torch.save(model, "proverb_classifier_full.pth")
print("Model saved!")

import torch

# Save model
torch.save(model.state_dict(), "proverb_classifier.pth")
print("Model saved!")