# -*- coding: utf-8 -*-
"""Afro LM YP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tEwW8dkd9fg7zxsCMV4R90YcQ--fFPtW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/drive/MyDrive/Combined_Proverbs.csv')

df.head(4)

# Checking for duplicate rows in the DataFrame
df.duplicated().sum()

# Drop duplicates rows
df = df.drop_duplicates()

df.duplicated().sum()

import re
from nltk.stem import PorterStemmer

# Custom Yoruba stopword list
yoruba_stopwords = ['ó', 'ní', 'ṣe', 'rẹ̀', 'tí', 'àwọn', 'sí', 'ni', 'náà',
                    'láti', 'kan', 'ti', 'ń', 'lọ', 'o', 'bí', 'padà', 'sì',
                    'wá', 'lè', 'wà', 'kí', 'púpọ̀', 'mi', 'wọ́n', 'pẹ̀lú',
                    'a', 'ṣùgbọ́n', 'fún', 'jẹ́', 'fẹ́', 'kò', 'jù', 'pé',
                    'é', 'gbogbo', 'inú', 'bẹ̀rẹ̀', 'jẹ', 'nítorí', 'nǹkan',
                    'sínú', 'yìí', 'ṣé', 'àti', 'í', 'máa', 'nígbà', 'mo',
                    'an', 'mọ̀', 'bá', 'kì', 'ńlá', 'ọ̀pọ̀lọpọ̀', 'ẹmọ́',
                    'wọn', 'òun']

stop_words = list(yoruba_stopwords)

# Initialize the stemmer
stemmer = PorterStemmer()

# Define function to remove URLs, HTML tags, stopwords, and apply stemming

def clean_text(text):
    # Check if input is a list and join into a string if necessary
    if isinstance(text, list):
        text = ' '.join(text)
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)

    # Tokenize the text
    words = text.split()

    # Remove stopwords and apply stemming
    filtered_and_stemmed_words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]

    # Join the filtered and stemmed words back into a sentence
    return ' '.join(filtered_and_stemmed_words)

# Apply the clean_text function to each row in the 'Proverbs' column
df['Proverbs'] = df['Proverbs'].apply(clean_text)

df.head()

# Save the cleaned dataframe to a new CSV file
df.to_csv('/content/drive/MyDrive/cleaned_Proverbs.csv', index=False)

import pandas as pd
from sklearn.model_selection import train_test_split

# Extract features and labels
texts = df['Proverbs']
labels = df['Labels']

# Train-test split
train_texts, test_texts, train_labels, test_labels = train_test_split(
    texts, labels, test_size=0.2, random_state=42
)



from transformers import BertTokenizer

# Load tokenizer
# Load model directly
from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("bonadossou/afrolm_active_learning")
model = AutoModelForMaskedLM.from_pretrained("bonadossou/afrolm_active_learning")

# Define tokenization function
def tokenize_texts(texts, tokenizer, max_len=50):
    return tokenizer(
        list(texts),
        add_special_tokens=True,
        max_length=max_len,
        padding="max_length",
        truncation=True,
        return_tensors="pt",
    )

# Tokenize training and testing data
train_encodings = tokenize_texts(train_texts, tokenizer)
test_encodings = tokenize_texts(test_texts, tokenizer)



import torch

class YorubaProverbDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: tensor[idx] for key, tensor in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)
        return item

# Create datasets
train_dataset = YorubaProverbDataset(train_encodings, train_labels)
test_dataset = YorubaProverbDataset(test_encodings, test_labels)

from transformers import AutoModel
import torch.nn as nn

class ProverbClassifier(nn.Module):
    def __init__(self, n_classes):
        super(ProverbClassifier, self).__init__()
        # Load the AfroLM model
        self.afro_lm = AutoModel.from_pretrained("bonadossou/afrolm_active_learning")
        self.drop = nn.Dropout(p=0.3)
        self.out = nn.Linear(self.afro_lm.config.hidden_size, n_classes)

    def forward(self, input_ids, attention_mask):
        # AfroLM outputs
        outputs = self.afro_lm(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        output = self.drop(pooled_output)
        return self.out(output)

model = ProverbClassifier(n_classes=2)

from torch.utils.data import DataLoader
from transformers import AdamW, get_scheduler
from torch.nn import CrossEntropyLoss
import torch

# DataLoaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

# Optimizer and scheduler
optimizer = AdamW(model.parameters(), lr=5e-5)
num_training_steps = len(train_loader) * 5  # 5 epochs
lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)

# Set device
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

# Define loss function
loss_fn = CrossEntropyLoss()

# Training loop
num_epochs = 5  # Updated to 5 epochs
for epoch in range(num_epochs):
    print(f"Epoch {epoch + 1}/{num_epochs}")
    model.train()
    epoch_loss = 0  # Track epoch loss

    for batch in train_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = loss_fn(outputs, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        lr_scheduler.step()

        # Accumulate loss
        epoch_loss += loss.item()

    avg_loss = epoch_loss / len(train_loader)  # Average loss for this epoch
    print(f"Average Loss: {avg_loss:.4f}")

print("Training complete!")

from sklearn.metrics import accuracy_score, classification_report

# Set model to evaluation mode
model.eval()

# Initialize variables to store predictions and true labels
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs, dim=1)  # Get predicted class

        # Store predictions and labels
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calculate accuracy
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy:.4f}")

# Generate classification report
print("Classification Report:")
print(classification_report(all_labels, all_preds, target_names=["Non-Proverb", "Proverb"]))

from sklearn.metrics import confusion_matrix

# Set model to evaluation mode
model.eval()

# Initialize variables to store predictions and true labels
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs, dim=1)  # Get predicted class

        # Store predictions and labels
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Generate confusion matrix
tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()

# Print the confusion matrix values
print(f"True Positive (TP): {tp}")
print(f"True Negative (TN): {tn}")
print(f"False Positive (FP): {fp}")
print(f"False Negative (FN): {fn}")

# Set model to evaluation mode after fine-tuning
model.eval()

# Initialize variables to store predictions and true labels
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        # Access the logits from the MaskedLMOutput
        logits = outputs.logits
        # Get the predictions by applying argmax to the logits
        preds = torch.argmax(logits, dim=1)  # Get predicted class


        # Store predictions and labels
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

#confusion matrix
tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()

# Print the confusion matrix values
print(f"True Positive (TP): {tp}")
print(f"True Negative (TN): {tn}")
print(f"False Positive (FP): {fp}")
print(f"False Negative (FN): {fn}")